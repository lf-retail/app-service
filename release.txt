using below workflow

name: release-was-warfileinstall
run-name: release-was-warfileinstall-${{ github.event.inputs.release_version }}

permissions:
  id-token: write
  contents: write
  security-events: write
  actions: read
  pull-requests: write

on:
  workflow_dispatch:
    inputs:
      release_version:
        description: 'Release version to deploy'
        required: true 
        type: string
      war_file_path:
        description: 'Path to the WAR file on the target server'
        required: true
        type: string
        default: '/ibm/IBM/WebSphere/AppServer/profiles/AppSrv01/installedApps/antibpsapp1Cell01/KYC_Remediation.war'
      war_file_name:
        description: 'Name of the WAR file'
        required: true
        type: string
        default: 'KYC_Remediation.war'
      app_name:
        description: 'Name of the deployed application'
        required: true
        type: string
        default: 'KYC_Remediation_war'
      context_root:
        description: 'Context root of the deployed application'
        required: true
        type: string
        default: '/KYC_Remediation'
        
jobs:
  IBPS-warfileinstall:
    uses: rakbank-internal/enterprise-reusable-workflows/.github/workflows/was-warfileinstall.yml@feature/optimum-deh-mcf-templates
    with:
      release_version: ${{ github.event.inputs.release_version }}
      war_file_path: ${{ github.event.inputs.war_file_path }}
      war_file_name: ${{ github.event.inputs.war_file_name }}
      app_name: ${{ github.event.inputs.app_name }}
      context_root: ${{ github.event.inputs.context_root }}
    secrets:
      TOKEN_GITHUB: ${{ secrets.TOKEN_GITHUB }}
      USERNAME_GITHUB: ${{ secrets.USERNAME_GITHUB }}
      WAS_ADMIN_USER: ${{ secrets.WAS_ADMIN_USER }}
      WAS_ADMIN_PASSWORD: ${{ secrets.WAS_ADMIN_PASSWORD }}

========================================

using below reusable workflow

name: was-warfileinstall-reusable-workflow

permissions:
  id-token: write
  contents: write
  security-events: write
  actions: read
  pull-requests: write

on:
  workflow_call:
    inputs:
      release_version:
        description: 'Release version to deploy'
        required: true 
        type: string
      war_file_path:
        description: 'Path to the WAR file on the target server'
        required: true
        type: string
      war_file_name:
        description: 'Name of the WAR file'
        required: true
        type: string
      app_name:
        description: 'Name of the deployed application'
        required: true
        type: string
      context_root:
        description: 'Context root of the deployed application'
        required: true
        type: string

    secrets:
      TOKEN_GITHUB: 
        required: true
      USERNAME_GITHUB:
        required: true
      WAS_ADMIN_USER:
        required: true
      WAS_ADMIN_PASSWORD:
        required: true

jobs:
  deploy:
    runs-on:
     group: rakbank-self-hosted-runner
     labels: dehitdevtra1

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Checkout roles repository
      uses: actions/checkout@v4
      with:
        repository: rakbank-internal/enterprise-ansible-roles
        token: ${{ secrets.TOKEN_GITHUB }}
        path: ansible-roles
        ref: Feature      
      
    - name: Checkout reusable code
      uses: actions/checkout@v4
      with:
        repository: rakbank-internal/enterprise-reusable-workflows
        ref: feature/optimum-deh-mcf-templates
        path: reusable-workflows
        token: ${{ secrets.TOKEN_GITHUB }}  

    - name: Make war-install-in-was.sh executable
      run: |
        chmod +x scripts/war-install-in-was.sh    

    - name: war file installation in WAS
      uses: ./reusable-workflows/.github/actions/was-warfileinstall
      with:
        release_version: ${{ inputs.release_version }}
        war_file_path: ${{ inputs.war_file_path }}
        war_file_name: ${{ inputs.war_file_name }}
        app_name: ${{ inputs.app_name }}
        context_root: ${{ inputs.context_root }}
        TOKEN_GITHUB: ${{ secrets.TOKEN_GITHUB }}
        USERNAME_GITHUB: ${{ secrets.USERNAME_GITHUB }}
        WAS_ADMIN_USER: ${{ secrets.WAS_ADMIN_USER }}
        WAS_ADMIN_PASSWORD: ${{ secrets.WAS_ADMIN_PASSWORD }}
================================================
using below composite action
name: 'was-warfileinstall'
description: 'was-warfileinstall'
inputs:
  release_version:
    description: 'Release version to deploy'
    required: true 
    type: string
  war_file_path:
    description: 'Path to the WAR file on the target server'
    required: true
    type: string
  war_file_name:
    description: 'Name of the WAR file'
    required: true
    type: string
  app_name:
    description: 'Name of the deployed application'
    required: true
    type: string
  context_root:
    description: 'Context root of the deployed application'
    required: true
    type: string
  TOKEN_GITHUB:
    description: 'GitHub token'
    required: true
    type: string
  USERNAME_GITHUB:
    description: 'GitHub username'
    required: true
    type: string    
  WAS_ADMIN_USER:
    description: 'GitHub username'
    required: true
    type: string
  WAS_ADMIN_PASSWORD:
    description: 'GitHub username'
    required: true
    type: string
  
runs:
  using: "composite"
  steps:
    - name: Run Ansible Playbook
      env:
        TOKEN_GITHUB: ${{ inputs.TOKEN_GITHUB }}
        release_version: ${{ inputs.release_version }}
        war_file_path: ${{ inputs.war_file_path }}
        war_file_name: ${{ inputs.war_file_name }}
        app_name: ${{ inputs.app_name }}
        context_root: ${{ inputs.context_root }}
        USERNAME_GITHUB: ${{ inputs.USERNAME_GITHUB }}
        WAS_ADMIN_USER: ${{ inputs.WAS_ADMIN_USER }}
        WAS_ADMIN_PASSWORD: ${{ inputs.WAS_ADMIN_PASSWORD }}
      run: |
        cd ${{ github.workspace }}
        #ansible-playbook -vvv -b --extra-vars "target=target_wasuat209 destination=${{ github.workspace }} release_version=${release_version} war_file_path=${war_file_path} war_file_name=${war_file_name} app_name=${app_name} context_root=${context_root} WAS_ADMIN_USER=${WAS_ADMIN_USER} WAS_ADMIN_PASSWORD=${WAS_ADMIN_PASSWORD} TOKEN_GITHUB=${TOKEN_GITHUB}" ./playbook/was-warfileinstall.yml #Cluster ip:10.15.11.209
        ansible-playbook -vvv -b --extra-vars "target=target_wasuat209 destination=${{ github.workspace }} release_version=${release_version} war_file_path=${war_file_path} war_file_name=${war_file_name} app_name=${app_name} context_root=${context_root} WAS_ADMIN_USER=${WAS_ADMIN_USER} WAS_ADMIN_PASSWORD=${WAS_ADMIN_PASSWORD} TOKEN_GITHUB=${TOKEN_GITHUB}" ./playbook/was-warfileinstall.yml #Cluster ip:10.15.11.209
        #ansible-playbook -vvv -b --extra-vars "target=target_wasuat209 destination=${{ github.workspace }} release_version=${{ env.release_version }} war_file_path=${{ env.war_file_path }} war_file_name=${{ env.war_file_name }} app_name=${{ env.app_name }} context_root=${{ env.context_root }} WAS_ADMIN_USER=${{ env.WAS_ADMIN_USER }} WAS_ADMIN_PASSWORD=${{ env.WAS_ADMIN_PASSWORD }}" ./playbook/was-warfileinstall.yml
      shell: bash   
=========================================================
using below playbook 

- name: was deploy warfileinstall in was
  hosts: '{{ target }}'
  become: yes
  become_user: itdevtra

  tasks:
    - name: Include was-warfile-deploy
      include_role:
        name:  was-warfile-deploy
        tasks_from: main.yml
      vars:
        github_token: "{{ lookup('env', 'TOKEN_GITHUB') }}"
        owner: "rakbank-internal"
        repo: "ibps-was-ansible-cd"
        branch: "IBPS-APP-ROLE"
        release_version: "{{ lookup('env', 'release_version') }}"
        Profile_home: "/ibm/IBM/Application/release"
        WAR_FILE_PATH:  "{{ lookup('env', 'war_file_path') }}"
        WAR_FILE_NAME:  "{{ lookup('env', 'war_file_name') }}"
        APP_NAME:  "{{ lookup('env', 'app_name') }}"
        CONTEXT_ROOT:  "{{ lookup('env', 'context_root') }}"
        WAS_HOST: "10.15.11.209"
        WAS_PORT: "8879"
        WAS_ADMIN_USER: "{{ lookup('env', 'WAS_ADMIN_USER') }}"
        WAS_ADMIN_PASSWORD: "{{ lookup('env', 'WAS_ADMIN_PASSWORD') }}"
        #WAS_ADMIN_USER: "deployer"
        #WAS_ADMIN_PASSWORD: "deployer@123"
        CELL_NAME: "antibpsapp1Cell01"
        CLUSTER_NAME: "BPMCLUSTER"
        WSADMIN: "/ibm/IBM/WebSphere/AppServer/profiles/AppSrv01/bin/wsadmin.sh"        
===========================================
using below ansible configuration

[defaults]
host_key_checking=False
deprecation_warnings=False
ansible_ssh_common_args='-o StrictHostKeyChecking=no'
ansible_ssh_extra_args='-o StrictHostKeyChecking=no'
remote_tmp=/tmp
inventory=hosts.cfg
roles_path=ansible-roles
comment_warnings=False
command_warnings=False
interpreter_python=auto
ANSIBLE_DEPRECATION_WARNINGS=False
ANSIBLE_COMMAND_WARNINGS=False
allow_world_readable_tmpfiles=yes
timeout=30
ansible_pipelining=True

========================================
using below host configuration

[all:vars]
ansible_user=itdevtra
ansible_ssh_port=22

#[target_wasuat148]
#ANT3CASAPPS01 ansible_ssh_host=10.15.13.148

[target_wasuat209]
ANTIBPSAPP1 ansible_ssh_host=10.15.11.209

#[target_wasuat210]
#ANTIBPSAPP1 ansible_ssh_host=10.15.11.210

#[target_wasreplica86]
#ANT3CASAPPS02 ansible_ssh_host=10.15.24.86
=================================
using below ansible roles

---
- name: Copy war-install-in-was.sh to Target Server
  become: yes
  become_user: itdevtra
  copy:
    #src: "/ansible/GITHUB_RUNNER/actions-runner10/_work/ibps-was-ansible-cd/ibps-was-ansible-cd/war-install-in-was.sh"
    src: "{{ destination }}/scripts/war-install-in-was.sh"
    dest: "{{ Profile_home }}/release.{{ release_version }}/war-install-in-was.sh"
    mode: '0755'

- name: Run war-install-in-was.sh on Target Server
  become: yes
  become_user: itdevtra
  shell: "cd {{ Profile_home }}/release.{{ release_version }} && ./war-install-in-was.sh {{ WAS_HOST }} {{ WAS_PORT }} {{ WAS_ADMIN_USER }} {{ WAS_ADMIN_PASSWORD }} {{ WAR_FILE_PATH }} {{ APP_NAME }} {{ CONTEXT_ROOT }} {{ CELL_NAME }} {{ CLUSTER_NAME }} {{ WSADMIN }} {{ WAR_FILE_NAME }}"
  register: deploy_result
  ignore_errors: yes
================================
using below ansible script for war file installation

#!/bin/bash
 
# Server details
WAS_HOST="$1"
WAS_PORT="$2"
WAS_ADMIN_USER="$3"
WAS_ADMIN_PASSWORD="$4"
 
# WAR file and application details
WAR_FILE_PATH="$5"
APP_NAME="$6"
CONTEXT_ROOT="$7"
CELL_NAME="$8"
CLUSTER_NAME="$9"
 
# Path to wsadmin.sh
WSADMIN="${10}"
WAR_FILE_NAME="${11}"

# Define the shared libraries array
shared_libs=("ibps_library" "omnidocs_library" "wfs_library")
 
# Create a new file for the Jython script
WSADMIN_SCRIPT="wsadmin_script.jy"
 
# Write the initial part of the Jython script to the file
cat <<EOF >$WSADMIN_SCRIPT
import time
import sys

# Check if the application is already running
if '${APP_NAME}' in AdminControl.queryNames('type=Application,*').splitlines():
    print 'Application ${APP_NAME} is already running. Stopping it...'
    app_managers = AdminControl.queryNames('type=ApplicationManager,*').splitlines()
    for app_manager in app_managers:
        AdminControl.invoke(app_manager, 'stopApplication', '${APP_NAME}', '[${CELL_NAME},${CLUSTER_NAME}]')
    print 'Application ${APP_NAME} stop command issued.'

# Wait for the application to stop
while '${APP_NAME}' in AdminControl.queryNames('type=Application,*').splitlines():
    print 'Waiting for application %s to stop...' % '${APP_NAME}'
    time.sleep(30)

print 'Application %s is not running.' % '${APP_NAME}'

# Uninstall the application if it already exists
if '${APP_NAME}' in AdminApp.list().splitlines():
    print 'Application %s already exists. Removing...' % '${APP_NAME}'
    result = AdminApp.uninstall('${APP_NAME}')
    if result != '':
        print 'Failed to uninstall application ' + '${APP_NAME}' + '. Error: ' + result
        raise Exception('Failed to uninstall application ' + '${APP_NAME}')
    else:
        AdminConfig.save()
        print 'Application %s removed.' % '${APP_NAME}'

# Install the application
result = AdminApp.install("${WAR_FILE_PATH}", '[ -nopreCompileJSPs -distributeApp -nouseMetaDataFromBinary -nodeployejb -appname ${APP_NAME} -createMBeansForResources -noreloadEnabled -nodeployws -validateinstall warn -noprocessEmbeddedConfig -filepermission .*\.dll=755#.*\.so=755#.*\.a=755#.*\.sl=755 -noallowDispatchRemoteInclude -noallowServiceRemoteInclude -asyncRequestDispatchType DISABLED -nouseAutoLink -noenableClientModule -clientMode isolated -novalidateSchema -contextroot ${CONTEXT_ROOT} -MapModulesToServers [[ ${WAR_FILE_NAME} ${WAR_FILE_NAME},WEB-INF/web.xml WebSphere:cell=${CELL_NAME},cluster=${CLUSTER_NAME} ]] -MapSharedLibForMod [[ ${APP_NAME} META-INF/application.xml wfs_library+omnidocs_library+ibps_library ]]')

if result != '':
    print 'Failed to install application ' + '${APP_NAME}' + '. Error: ' + result
    raise Exception('Failed to install application ' + '${APP_NAME}')
    
# Set the class loader policy to PARENT_LAST
dep = AdminConfig.getid('/Deployment:${APP_NAME}/')
depObject = AdminConfig.showAttribute(dep, 'deployedObject')

modules = AdminConfig.showAttribute(depObject, 'modules')
for module in modules[1:-1].split():
    if AdminConfig.showAttribute(module, 'uri') == 'KYC_Remediation.war':
        warClassldr = AdminConfig.showAttribute(module, 'classloader')
        AdminConfig.modify(warClassldr, [['mode', 'PARENT_LAST']])
        break

# Save configuration and add a delay before starting the application
AdminConfig.save()
time.sleep(60)

# Start the application on each node in the cluster
print 'Starting application ${APP_NAME}'
AdminControl.invoke('WebSphere:name=ApplicationManager,process=BPMMBR01,platform=proxy,node=antibpsapp1Node01,version=8.5.5.25,type=ApplicationManager,mbeanIdentifier=ApplicationManager,cell=${CELL_NAME},spec=1.0', 'startApplication', '${APP_NAME}')
AdminControl.invoke('WebSphere:name=ApplicationManager,process=BPMMBR02,platform=proxy,node=antibpsapp2Node01,version=8.5.5.25,type=ApplicationManager,mbeanIdentifier=ApplicationManager,cell=${CELL_NAME},spec=1.0', 'startApplication', '${APP_NAME}')
print 'Application ${APP_NAME} started.'
EOF

# Execute the script with wsadmin
$WSADMIN -lang jython -conntype SOAP -host $WAS_HOST -port $WAS_PORT -user $WAS_ADMIN_USER -password $WAS_ADMIN_PASSWORD -f $WSADMIN_SCRIPT
==========================

in ansible playbook i am uisng 
WAS_HOST: "10.15.11.209"
WAS_PORT: "8879"

but in my application i am using 3 ip address in host configuration because the WAS_HOST and WAS_PORT its dynamic it will be change so how do i parameter and pass it to my playbook default using host configuration
